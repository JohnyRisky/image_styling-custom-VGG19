{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1908a3d3-4449-4b33-b385-fc95ab124bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import models\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eaf940cd-6b54-4ce2-af32-f87926b79868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00cf976-5fc5-4011-bcf5-1bd0598e79ce",
   "metadata": {},
   "source": [
    "### Images preparation stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "6905abe9-cc59-43b4-b222-a7d3512fecfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = Image.open(\"image.jpeg\").convert('RGB') # A puppy picture # RGB as we send 3 channels to CNN model\n",
    "image2 = Image.open(\"vahn_gogh.png\").convert('RGB') # one of the great Vahn Gogh's pictures\n",
    "\n",
    "v2_transforms = v2.Compose([v2.Resize(256),\n",
    "                            v2.CenterCrop(224),\n",
    "                            v2.ToImage(), \n",
    "                            v2.ToDtype(torch.float32, scale=True),\n",
    "                            v2.Lambda(lambda img: img.unsqueeze(0))])\n",
    "\n",
    "image1, image2 = [v2_transforms(img).to(device) for img in (image1, image2)] # tranforming both images into tensors shaped like 1-224-224 where 1 is for batch and next two are for sizes\n",
    "\n",
    "image1_clone = image1.clone().requires_grad_(True).to(device) # creating image1 clone for following styling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936b1b5c-140f-4d10-87bc-8c9da59d0fae",
   "metadata": {},
   "source": [
    "### Custom VGG19 model preparation stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "68b3053d-0fff-430d-911d-200a9f603db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomVGG(nn.Module):\n",
    "    def __init__(self, layers: list):\n",
    "        super().__init__()\n",
    "        self.vgg19 = models.vgg19(weights=models.VGG19_Weights.DEFAULT)\n",
    "        self.features = self.vgg19.features\n",
    "        self.features.requires_grad_(False)\n",
    "        self.requires_grad_(False)\n",
    "        self.features.eval()  # так как в классе есть dropout, надо включать модель\n",
    "        self.layers = layers\n",
    "        \n",
    "    def forward(self, x):\n",
    "        results = []\n",
    "        for ind, layer in enumerate(self.features):\n",
    "            x = layer(x)\n",
    "            if ind in self.layers:\n",
    "                results.append(x.squeeze())\n",
    "        return results\n",
    "\n",
    "model = MyCustomVGG([0, 5, 10, 19, 28, 34]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "c1a402ed-28e5-42ee-9f0b-c9ab8c5a7dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image1.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ca6a41-000f-4369-a919-ccb5689c6aea",
   "metadata": {},
   "source": [
    "### Supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "8f45fb62-387b-4e6c-9f3d-9a91d5bb8d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(orig_img, orig_img_clone):\n",
    "    return torch.mean((orig_img - orig_img_clone) ** 2)\n",
    "\n",
    "# def gram_matrix(tensor: torch.Tensor):\n",
    "#     if tensor.dim() == 4:\n",
    "#         tensor = tensor.squeeze(0)  # если есть размер батча\n",
    "\n",
    "#     C, H, W = tensor.size()\n",
    "#     features = tensor.view(C, H * W)\n",
    "#     gram = torch.mm(features, features.t())\n",
    "\n",
    "#     return gram / (C * H * W)\n",
    "\n",
    "def gram_matrix(tensor):\n",
    "    filters = tensor.size(dim=0)\n",
    "    g = tensor.view(filters, -1)\n",
    "    gram = torch.mm(g, g.mT) / g.size(dim=1)\n",
    "\n",
    "    return gram\n",
    "\n",
    "def style_loss(clone_style_tensors, image2_grams):\n",
    "    weights = [1, 0.8, 0.5, 0.3, 0.1]\n",
    "\n",
    "    loss = 0\n",
    "    i = 0\n",
    "    for base, target in zip(clone_style_tensors, image2_grams):\n",
    "        gram = gram_matrix(base)\n",
    "        loss += weights[i] * torch.mean((gram - target) ** 2)\n",
    "        i += 1\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "3b7f24b7-47c7-461f-b179-545362de529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1_vggcustomed = model(image1) # image1 and image2 processed by the model\n",
    "image2_vggcustomed = model(image2)\n",
    "\n",
    "image2_gram = [gram_matrix(x) for x in image2_vggcustomed[:5]]\n",
    "\n",
    "content_weight = 1\n",
    "style_weight = 1000\n",
    "best_loss = -1\n",
    "epochs = 100\n",
    "best_img = image1_clone.clone()\n",
    "\n",
    "optimizer = optim.Adam(params=[image1_clone], lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "e10caa08-852c-4892-b1c8-99d8b3ec0d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, loss: 578.6679\n",
      "Iteration: 1, loss: 544.0251\n",
      "Iteration: 2, loss: 378.4812\n",
      "Iteration: 3, loss: 585.8953\n",
      "Iteration: 4, loss: 679.1354\n",
      "Iteration: 5, loss: 533.3723\n",
      "Iteration: 6, loss: 558.8918\n",
      "Iteration: 7, loss: 493.2389\n",
      "Iteration: 8, loss: 458.6534\n",
      "Iteration: 9, loss: 424.7810\n",
      "Iteration: 10, loss: 388.7843\n",
      "Iteration: 11, loss: 361.5681\n",
      "Iteration: 12, loss: 329.0883\n",
      "Iteration: 13, loss: 304.3185\n",
      "Iteration: 14, loss: 280.7676\n",
      "Iteration: 15, loss: 258.5783\n",
      "Iteration: 16, loss: 239.1976\n",
      "Iteration: 17, loss: 220.5872\n",
      "Iteration: 18, loss: 202.9990\n",
      "Iteration: 19, loss: 187.7311\n",
      "Iteration: 20, loss: 174.0888\n",
      "Iteration: 21, loss: 160.2138\n",
      "Iteration: 22, loss: 148.2854\n",
      "Iteration: 23, loss: 137.0153\n",
      "Iteration: 24, loss: 126.6022\n",
      "Iteration: 25, loss: 116.9087\n",
      "Iteration: 26, loss: 108.0686\n",
      "Iteration: 27, loss: 101.4310\n",
      "Iteration: 28, loss: 101.4318\n",
      "Iteration: 29, loss: 132.9100\n",
      "Iteration: 30, loss: 85.3831\n",
      "Iteration: 31, loss: 97.3374\n",
      "Iteration: 32, loss: 112.0142\n",
      "Iteration: 33, loss: 85.2874\n",
      "Iteration: 34, loss: 98.3323\n",
      "Iteration: 35, loss: 92.6776\n",
      "Iteration: 36, loss: 80.2208\n",
      "Iteration: 37, loss: 80.2507\n",
      "Iteration: 38, loss: 83.0027\n",
      "Iteration: 39, loss: 69.2635\n",
      "Iteration: 40, loss: 72.0102\n",
      "Iteration: 41, loss: 74.5674\n",
      "Iteration: 42, loss: 57.6496\n",
      "Iteration: 43, loss: 64.6669\n",
      "Iteration: 44, loss: 60.5719\n",
      "Iteration: 45, loss: 54.7820\n",
      "Iteration: 46, loss: 53.8907\n",
      "Iteration: 47, loss: 43.5990\n",
      "Iteration: 48, loss: 47.1966\n",
      "Iteration: 49, loss: 44.3816\n",
      "Iteration: 50, loss: 47.0401\n",
      "Iteration: 51, loss: 70.3700\n",
      "Iteration: 52, loss: 40.6098\n",
      "Iteration: 53, loss: 52.4622\n",
      "Iteration: 54, loss: 50.6424\n",
      "Iteration: 55, loss: 50.6617\n",
      "Iteration: 56, loss: 49.6704\n",
      "Iteration: 57, loss: 40.0792\n",
      "Iteration: 58, loss: 42.3456\n",
      "Iteration: 59, loss: 37.8126\n",
      "Iteration: 60, loss: 46.0796\n",
      "Iteration: 61, loss: 71.4831\n",
      "Iteration: 62, loss: 46.3501\n",
      "Iteration: 63, loss: 44.1376\n",
      "Iteration: 64, loss: 38.1842\n",
      "Iteration: 65, loss: 46.2334\n",
      "Iteration: 66, loss: 51.4252\n",
      "Iteration: 67, loss: 44.5830\n",
      "Iteration: 68, loss: 51.5980\n",
      "Iteration: 69, loss: 34.9162\n",
      "Iteration: 70, loss: 44.1501\n",
      "Iteration: 71, loss: 44.9671\n",
      "Iteration: 72, loss: 58.5802\n",
      "Iteration: 73, loss: 84.9075\n",
      "Iteration: 74, loss: 52.1047\n",
      "Iteration: 75, loss: 64.8105\n",
      "Iteration: 76, loss: 69.8542\n",
      "Iteration: 77, loss: 78.1793\n",
      "Iteration: 78, loss: 88.7291\n",
      "Iteration: 79, loss: 72.3189\n",
      "Iteration: 80, loss: 70.9059\n",
      "Iteration: 81, loss: 61.9749\n",
      "Iteration: 82, loss: 66.0070\n",
      "Iteration: 83, loss: 69.2027\n",
      "Iteration: 84, loss: 70.5951\n",
      "Iteration: 85, loss: 85.9953\n",
      "Iteration: 86, loss: 60.8416\n",
      "Iteration: 87, loss: 61.4006\n",
      "Iteration: 88, loss: 62.9805\n",
      "Iteration: 89, loss: 55.2250\n",
      "Iteration: 90, loss: 48.2056\n",
      "Iteration: 91, loss: 47.3854\n",
      "Iteration: 92, loss: 42.7694\n",
      "Iteration: 93, loss: 40.9834\n",
      "Iteration: 94, loss: 39.8485\n",
      "Iteration: 95, loss: 57.0763\n",
      "Iteration: 96, loss: 45.7552\n",
      "Iteration: 97, loss: 51.7791\n",
      "Iteration: 98, loss: 33.6388\n",
      "Iteration: 99, loss: 44.6720\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    results_image1_clone = model(image1_clone)\n",
    "\n",
    "    cl = content_loss(results_image1_clone[-1], image1_vggcustomed[-1])\n",
    "    sl = style_loss(results_image1_clone, image2_gram)\n",
    "    loss = (content_weight * cl) + (style_weight * sl)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    image1_clone.data.clamp_(0, 1)\n",
    "\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_img = image1_clone.clone()\n",
    "\n",
    "    print(f\"Iteration: {epoch}, loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "58050594-271c-40ab-ac9a-cbf71d7c8987",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = best_img.detach().squeeze()\n",
    "low, hi = torch.amin(x), torch.amax(x)\n",
    "x = (x - low) / (hi - low) * 255.0\n",
    "x = x.permute(1, 2, 0)\n",
    "x = x.cpu().numpy()\n",
    "x = np.clip(x, 0, 255).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "6fb5830d-d945-4a0c-85e1-a13170d6c017",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.fromarray(x, 'RGB')\n",
    "image.save('result.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44460099-ce7d-4db9-af96-037b5719b457",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
